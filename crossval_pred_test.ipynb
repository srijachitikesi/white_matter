{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from cust_dataset import CustDataset\n",
    "from network import Network\n",
    "from torch.utils import *\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CustomDataset 1\n",
      "Number of rows selected: 4000\n",
      "Loaded labels...\n",
      "sss done\n",
      "CustomDataset initialized.\n"
     ]
    }
   ],
   "source": [
    "data = CustDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.117182928739916 3.663844091250903\n"
     ]
    }
   ],
   "source": [
    "max = np.max(data.vars['new_score'])\n",
    "min = np.min(data.vars['new_score'])\n",
    "print(min, max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_default_dtype(torch.float32)\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('job_id',type=str)\n",
    "# args = parser.parse_args()\n",
    "# print(args.job_id)\n",
    "# print('number of gpus ',torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory = args.job_id\n",
    "# parent_directory = '/data/users2/pnadigapusuresh1/JobOutputs'\n",
    "# path = os.path.join(parent_directory,directory)\n",
    "# model_save_path = os.path.join(path,'models_fold')\n",
    "\n",
    "# if not os.path.exists(path):\n",
    "#     os.mkdir(path)\n",
    "#     os.mkdir(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(52)\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 4\n",
    "# how many samples per batch to load\n",
    "batch_size = 15\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     batch_size *= torch.cuda.device_count()\n",
    "# else:\n",
    "#     batch_size = 15\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.20\n",
    "# percentage of data to be used for testset\n",
    "test_size = 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CustomDataset 1\n",
      "Number of rows selected: 500\n",
      "data src_subject_id\n",
      "NDARINV005V6D2C   -2.640752\n",
      "NDARINV007W6H7B    1.337910\n",
      "NDARINV00CY2MDM    1.058453\n",
      "NDARINV00HEV6HB    0.508555\n",
      "NDARINV00LJVZK2    0.509983\n",
      "                     ...   \n",
      "NDARINV1VMB1ZYW    3.450872\n",
      "NDARINV1VMJZV9V   -0.359201\n",
      "NDARINV1VRGHTD2   -1.741937\n",
      "NDARINV1VXVRPHJ    0.512647\n",
      "NDARINV1W2D4CFP   -0.189944\n",
      "Name: new_score, Length: 500, dtype: float64\n",
      "Loaded labels...\n",
      "sss done\n",
      "CustomDataset initialized.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_data = CustDataset(train=False,valid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cust_dataset.CustDataset at 0x7f871d48eb10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered variables\n",
    "vars = test_data.vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for k-fold\n",
    "\n",
    "sss = ShuffleSplit(n_splits=5,test_size=0.2,random_state=52)\n",
    "learning_rate = 0.00001\n",
    "fold = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 27, 103,  88,  83,  55,   4,  44,  62,  93,  60,  14,  50,  66,\n",
       "         29,  95,  41,  58, 114,  77,  37,  96, 112, 111,  82,   3, 113,\n",
       "         57,  89, 101,  67, 105, 100,  84,  12,  56,  81,  98,  53,   1,\n",
       "        116,  10,  80,  17,   9,  49,  30,  38,   0,  26,  15,  25,   6,\n",
       "         65,  85,  59,  64,  74,  78,  43,  34,  20, 108,   7,  90,  71,\n",
       "         22,  39,  63,  76, 104,  79,  45,  61,  42,  46,  54, 109,  16,\n",
       "          5,  33, 110,  97,  99,  35,  91, 119, 106,  69,  32, 118,  86,\n",
       "         23,  13,  11,  28, 117]),\n",
       " array([107,  52, 102,  18,  94,  47,  36, 115,  92,  68,  70,   2,  19,\n",
       "         24,  72,  21,  40,  75,  73,  87,   8,  31,  48,  51]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(sss.split(test_data.train_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = SubsetRandomSampler(test_data.train_idx)\n",
    "valid_sampler = SubsetRandomSampler(test_data.test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(test_data,batch_size=batch_size, \n",
    "                                sampler= train_sampler, num_workers=num_workers)\n",
    "valid_loader = DataLoader(test_data,batch_size=batch_size, \n",
    "                                sampler= valid_sampler, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(torch.unsqueeze(X, 1).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2712],\n",
       "        [-0.2299],\n",
       "        [-0.2923],\n",
       "        [-0.2592],\n",
       "        [-0.2562]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Network(\n",
      "  (cv1): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "  (bn1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (cv2): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "  (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (cv3): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "  (bn3): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (cv4): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "  (bn4): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (cv5): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "  (bn5): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=9216, out_features=1, bias=True)\n",
      "  (d3d): Dropout3d(p=0.2, inplace=False)\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "    (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "    (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (layer5): Sequential(\n",
      "    (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "    (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (convs): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "      (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "      (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "      (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "      (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "      (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Starting to Train...\n",
      "train size 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid size 1\n",
      "Epoch: 1/10.. \n",
      "Train Loss 3.2995426654815674 1\n",
      "Validation Loss 1.3055212497711182 1\n",
      "train size 1\n",
      "Valid size 1\n",
      "Epoch: 2/10.. \n",
      "Train Loss 166.49395751953125 2\n",
      "Validation Loss 0.6933380365371704 2\n",
      "train size 1\n",
      "Valid size 1\n",
      "Epoch: 3/10.. \n",
      "Train Loss 5.317201137542725 3\n",
      "Validation Loss 0.4049515426158905 3\n",
      "train size 1\n",
      "Valid size 1\n",
      "Epoch: 4/10.. \n",
      "Train Loss 44.45218276977539 4\n",
      "Validation Loss 0.3655177056789398 4\n",
      "train size 1\n",
      "Valid size 1\n",
      "Epoch: 5/10.. \n",
      "Train Loss 39.626830101013184 5\n",
      "Validation Loss 0.3829321265220642 5\n",
      "train size 1\n",
      "Valid size 1\n",
      "Epoch: 6/10.. \n",
      "Train Loss 20.568161487579346 6\n",
      "Validation Loss 0.4155452251434326 6\n",
      "train size 1\n",
      "Valid size 1\n",
      "Epoch: 7/10.. \n",
      "Train Loss 1.5559585690498352 7\n",
      "Validation Loss 0.41929036378860474 7\n",
      "train size 1\n",
      "Valid size 1\n",
      "Epoch: 8/10.. \n",
      "Train Loss 9.55005955696106 8\n",
      "Validation Loss 0.3845386207103729 8\n",
      "train size 1\n",
      "Valid size 1\n",
      "Epoch: 9/10.. \n",
      "Train Loss 6.065716981887817 9\n",
      "Validation Loss 0.33692383766174316 9\n",
      "train size 1\n",
      "Valid size 1\n",
      "Epoch: 10/10.. \n",
      "Train Loss 5.866066932678223 10\n",
      "Validation Loss 0.2685590982437134 10\n",
      "####################################################################\n",
      "Done\n",
      "Using cpu device\n",
      "Network(\n",
      "  (cv1): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "  (bn1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (cv2): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "  (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (cv3): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "  (bn3): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (cv4): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "  (bn4): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (cv5): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "  (bn5): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=9216, out_features=1, bias=True)\n",
      "  (d3d): Dropout3d(p=0.2, inplace=False)\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "    (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "    (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (layer5): Sequential(\n",
      "    (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "    (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (convs): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "      (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "      (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "      (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "      (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "      (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Starting to Train...\n",
      "train size 1\n",
      "Valid size 1\n",
      "Epoch: 1/10.. \n",
      "Train Loss 1.3395821750164032 1\n",
      "Validation Loss 0.6867883205413818 1\n",
      "train size 1\n",
      "Valid size 1\n",
      "Epoch: 2/10.. \n",
      "Train Loss 45.468955993652344 2\n",
      "Validation Loss 0.3077409565448761 2\n",
      "train size 1\n",
      "Valid size 1\n",
      "Epoch: 3/10.. \n",
      "Train Loss 21.094420909881592 3\n",
      "Validation Loss 0.3139650523662567 3\n",
      "train size 1\n",
      "Valid size 1\n",
      "Epoch: 4/10.. \n",
      "Train Loss 9.42224907875061 4\n",
      "Validation Loss 0.418117493391037 4\n",
      "train size 1\n",
      "Valid size 1\n",
      "Epoch: 5/10.. \n",
      "Train Loss 12.774506092071533 5\n",
      "Validation Loss 0.44101887941360474 5\n",
      "train size 1\n",
      "Valid size 1\n",
      "Epoch: 6/10.. \n",
      "Train Loss 6.642110824584961 6\n",
      "Validation Loss 0.4041872024536133 6\n",
      "train size 1\n",
      "Valid size 1\n",
      "Epoch: 7/10.. \n",
      "Train Loss 0.33754098415374756 7\n",
      "Validation Loss 0.37250977754592896 7\n",
      "train size 1\n",
      "Valid size 1\n",
      "Epoch: 8/10.. \n",
      "Train Loss 3.505623936653137 8\n",
      "Validation Loss 0.37581419944763184 8\n",
      "train size 1\n",
      "Valid size 1\n",
      "Epoch: 9/10.. \n",
      "Train Loss 5.653690695762634 9\n",
      "Validation Loss 0.43010276556015015 9\n",
      "train size 1\n",
      "Valid size 1\n",
      "Epoch: 10/10.. \n",
      "Train Loss 2.8526129722595215 10\n",
      "Validation Loss 0.5219131708145142 10\n",
      "####################################################################\n",
      "Done\n",
      "Using cpu device\n",
      "Network(\n",
      "  (cv1): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "  (bn1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (cv2): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "  (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (cv3): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "  (bn3): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (cv4): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "  (bn4): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (cv5): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "  (bn5): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=9216, out_features=1, bias=True)\n",
      "  (d3d): Dropout3d(p=0.2, inplace=False)\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "    (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "    (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (layer5): Sequential(\n",
      "    (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "    (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (convs): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "      (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "      (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "      (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "      (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "      (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Starting to Train...\n",
      "train size 1\n",
      "Valid size 1\n",
      "Epoch: 1/10.. \n",
      "Train Loss 0.9153275191783905 1\n",
      "Validation Loss 0.6054395437240601 1\n",
      "train size 1\n",
      "Valid size 1\n",
      "Epoch: 2/10.. \n",
      "Train Loss 92.15629577636719 2\n",
      "Validation Loss 0.306121289730072 2\n",
      "train size 1\n",
      "Valid size 1\n",
      "Epoch: 3/10.. \n",
      "Train Loss 23.34966230392456 3\n",
      "Validation Loss 0.24199698865413666 3\n",
      "train size 1\n",
      "Valid size 1\n",
      "Epoch: 4/10.. \n",
      "Train Loss 7.334697961807251 4\n",
      "Validation Loss 0.1893063485622406 4\n",
      "train size 1\n",
      "Valid size 1\n",
      "Epoch: 5/10.. \n",
      "Train Loss 26.63723659515381 5\n",
      "Validation Loss 0.12880459427833557 5\n",
      "train size 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/multiprocessing/connection.py:256\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/multiprocessing/connection.py:423\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 423\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/multiprocessing/connection.py:930\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    927\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 930\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    932\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for train_idx, valid_idx in sss.split(train_data.train_idx):\n",
    "    # writer = SummaryWriter(log_dir=path+'/fold'+str(fold))\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_data.test_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_data,batch_size=batch_size, \n",
    "                                sampler= train_sampler, num_workers=num_workers)\n",
    "    valid_loader = DataLoader(valid_data,batch_size=batch_size, \n",
    "                                sampler= valid_sampler, num_workers=num_workers)\n",
    "\n",
    "    test_loader = DataLoader(test_data,batch_size=batch_size,\n",
    "                                sampler= test_sampler, num_workers=num_workers)\n",
    "\n",
    "    device = (\n",
    "        \"mps\"\n",
    "        if torch.backends.mps.is_available()\n",
    "        else \"cpu\"\n",
    "    )\n",
    "    print(f\"Using {device} device\")\n",
    "\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # print(f\"Using {device} device\")         \n",
    "\n",
    "    model = Network()\n",
    "   \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    #%%\n",
    "\n",
    "    epochs = 10\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    #%%\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    print('Starting to Train...')\n",
    "\n",
    "    for e in range(1,epochs+1):\n",
    "        size = len(train_loader)\n",
    "        print(\"train size\", size)\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        # with torch.no_grad():\n",
    "        for X, y in train_loader:\n",
    "            X, y = X.to(device).float(), y.to(device).float()\n",
    "            X = torch.unsqueeze(X, 1).float() \n",
    "            pred = model(X)\n",
    "            pred = pred.squeeze()  \n",
    "            loss = criterion(pred, y)\n",
    "            train_loss += ((loss.item())*X.shape[0])\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_idx)\n",
    "        # avg_train_loss = train_loss / len(train_loader)\n",
    "        # print(f\"Average train loss: {avg_train_loss}\")\n",
    "\n",
    "        #<!------Valid------->\n",
    "        # else:\n",
    "        size = len(valid_loader)\n",
    "        print(\"Valid size\", size)\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X,y in valid_loader:\n",
    "                X, y = X.to(device).float(), y.to(device).float()\n",
    "                X = torch.unsqueeze(X, 1).float() \n",
    "                pred = model(X)\n",
    "                pred = pred.squeeze()  \n",
    "                loss = criterion(pred, y)\n",
    "                valid_loss += ((loss.item())*X.shape[0])\n",
    "\n",
    "        avg_valid_loss = valid_loss / len(valid_idx)\n",
    "        # avg_valid_loss = valid_loss / len(valid_loader)\n",
    "        # print(f\"Average valid loss: {avg_valid_loss}\")\n",
    "\n",
    "\n",
    "        #<!------Test-------->\n",
    "        # size = len(test_loader)\n",
    "        # print(\"test size\", size)\n",
    "        # model.eval()\n",
    "        # test_loss = 0\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     for X, y in test_loader:\n",
    "        #         X, y = X.to(device).float(), y.to(device).float()\n",
    "        #         X = torch.unsqueeze(X, 1).float() \n",
    "        #         pred = model(X)\n",
    "        #         pred = pred.squeeze()  \n",
    "        #         test_loss += criterion(pred, y).item()\n",
    "\n",
    "        # avg_test_loss = test_loss / len(test_loader)\n",
    "        # print(f\"Average test loss: {avg_test_loss}\")\n",
    "\n",
    "        \n",
    "        print(\"Epoch: {}/{}.. \".format(e, epochs))\n",
    "\n",
    "        print('Train Loss', train_loss/len(train_loader),e)\n",
    "        print('Validation Loss', valid_loss/len(valid_loader),e)\n",
    "        # print('Test Loss', test_loss/len(test_loader),e)\n",
    "        \n",
    "\n",
    "    # fold+=1\n",
    "    print('####################################################################')\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
